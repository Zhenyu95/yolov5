{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "known-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.yolo import Detect\n",
    "from utils.general import scale_coords, non_max_suppression, xyxy2xywh, xyxy2xywhn\n",
    "import coremltools\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "average-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "# COREML_MODEL = \"/Users/zhenyu/Documents/Scripts/IphoneAOI/yolov5/best_1106.mlmodel\"\n",
    "COREML_MODEL = \"/Users/zhenyu/Desktop/exp6/weights/best.mlmodel\"\n",
    "IMAGE_FOLDER = \"/Users/zhenyu/Desktop/val/\"\n",
    "OUT_FOLDER = \"/Users/zhenyu/Desktop/test/\"\n",
    "SAVE_IMG = True\n",
    "VIEW_IMG = False\n",
    "SAVE_TXT = True\n",
    "CAT_NAMES = ['Screw', 'unknown']\n",
    "COLORS = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(CAT_NAMES))]\n",
    "PATH = \"./\"\n",
    "ANCHORS = ([116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]) # from <model>.yml\n",
    "IMG_SIZE = (2304, 3072)\n",
    "# IMG_SIZE = (2560, 2560)\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "nc = len(CAT_NAMES)\n",
    "nl = len(ANCHORS)\n",
    "na = len(ANCHORS[0]) // 2\n",
    "no = nc + 5  # number of outputs per anchor\n",
    "grid = [torch.zeros(1)] * nl  # init grid\n",
    "a = torch.tensor(ANCHORS).float().view(nl, -1, 2)\n",
    "anchor_grid = a.clone().view(nl, 1, -1, 1, 1, 2)\n",
    "# stride for 2304,3072\n",
    "stride = [32, 8, 16]\n",
    "# stride for 2560,2560\n",
    "# stride = [32, 64, 16]\n",
    "conf_thres = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "disabled-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, resize_to=None):\n",
    "    # resize_to: (Width, Height)\n",
    "    img = PIL.Image.open(path)\n",
    "    if resize_to is not None:\n",
    "        img = img.resize(resize_to, PIL.Image.ANTIALIAS)\n",
    "    img_np = np.array(img).astype(np.float32)\n",
    "    return img_np, img\n",
    "\n",
    "def make_grid(nx=20, ny=20):\n",
    "    yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
    "    return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n",
    "\n",
    "def resize_image(source_image):\n",
    "    background = PIL.Image.new('RGB', IMG_SIZE, \"black\")\n",
    "    source = source_image.copy()\n",
    "    source.thumbnail(IMG_SIZE)\n",
    "    (w, h) = source.size\n",
    "    background.paste(source, (int((IMG_SIZE[0] - w) / 2), int((IMG_SIZE[1] - h) / 2 )))\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tamil-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = PIL.Image.open(os.path.join(IMAGE_FOLDER, '2.jpg'))\n",
    "resized = resize_image(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "waiting-recommendation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3024, 4032)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "returning-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(file_name):   \n",
    "    source = PIL.Image.open(os.path.join(IMAGE_FOLDER, file_name))\n",
    "    resized = resize_image(source)\n",
    "\n",
    "    predictions = model.predict({'image': resized})\n",
    "\n",
    "    z = []  # inference output\n",
    "    x = []\n",
    "    for pred in predictions:\n",
    "        x.append(torch.Tensor(predictions[pred]))\n",
    "    x.reverse()\n",
    "\n",
    "    for i in range(nl):\n",
    "        bs, _, ny, nx, _ = x[i].shape\n",
    "\n",
    "        if grid[i].shape[2:4] != x[i].shape[2:4]:\n",
    "            grid[i] = make_grid(nx, ny)\n",
    "\n",
    "        y = x[i].sigmoid()\n",
    "        y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + grid[i]) * stride[i]  # xy\n",
    "        y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * anchor_grid[i]  # wh\n",
    "        z.append(y.view(bs, -1, no))\n",
    "    \n",
    "    pred = torch.cat(z, 1)\n",
    "\n",
    "    pred = non_max_suppression(pred, conf_thres, .3, classes=None, agnostic=False)\n",
    "\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        p, s = \"./\", \"\"\n",
    "        label = []\n",
    "\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            # det = det[((det[:, 0]-det[:, 2])*(det[:, 1]-det[:, 3])) > 80]\n",
    "\n",
    "            # Print results\n",
    "#             for c in det[:, -1].unique():\n",
    "#                 n = (det[:, -1] == c).sum()  # detections per class\n",
    "#                 s += '%g %ss, ' % (n, CAT_NAMES[int(c)])  # add to string\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in det:\n",
    "                if SAVE_TXT:\n",
    "                    xywh = xyxy2xywhn(torch.tensor(xyxy).view(1, 4), w=IMG_SIZE[0], h=IMG_SIZE[1]).view(-1).tolist()\n",
    "                    label.append(('%g ' * 5 + '\\n') % (cls, *xywh))\n",
    "                if SAVE_IMG:\n",
    "                    draw = PIL.ImageDraw.Draw(source)\n",
    "                    draw.rectangle(np.array(torch.tensor(xyxy).view(2,2)*1.3125), outline='red')\n",
    "        if SAVE_TXT:\n",
    "            with open(os.path.join(OUT_FOLDER, '{}.txt'.format(file_name[:-4])), 'a') as f:\n",
    "                for line in label:\n",
    "                    f.write(line)\n",
    "        if SAVE_IMG:\n",
    "            source.save(os.path.join(OUT_FOLDER, '{}.jpg'.format(file_name[:-4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "frequent-easter",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = coremltools.models.model.MLModel(COREML_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "tutorial-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval('P9YI6XTHXQ.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "artificial-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "second-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUT_FOLDER, 'dda.txt'), 'a') as f:\n",
    "    for line in x:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "hollow-zoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_1808:(1, 3, 48, 36, 7)\n",
      "var_1778:(1, 3, 192, 144, 7)\n",
      "var_1763:(1, 3, 384, 288, 7)\n",
      "var_1793:(1, 3, 96, 72, 7)\n"
     ]
    }
   ],
   "source": [
    "for each in predictions.keys():\n",
    "    print('{}:{}'.format(each, predictions[each].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "headed-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pred[0][((pred[0][:, 0]-pred[0][:, 2])*(pred[0][:, 1]-pred[0][:, 3])) > 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "basic-triangle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.45282e+03, 1.45009e+03, 1.50923e+03, 1.53559e+03, 8.58536e-01, 0.00000e+00],\n",
       "        [8.11149e+02, 2.51468e+02, 8.65843e+02, 3.34357e+02, 8.56946e-01, 0.00000e+00],\n",
       "        [1.32890e+03, 6.14181e+02, 1.38540e+03, 6.95824e+02, 8.49743e-01, 0.00000e+00],\n",
       "        [1.32848e+03, 1.39324e+03, 1.36927e+03, 1.45567e+03, 8.44309e-01, 0.00000e+00],\n",
       "        [8.95427e+02, 1.55035e+02, 9.40290e+02, 2.22714e+02, 8.43017e-01, 0.00000e+00],\n",
       "        [8.04034e+02, 4.29946e+02, 8.54911e+02, 5.09081e+02, 8.40511e-01, 0.00000e+00],\n",
       "        [1.42461e+03, 2.20768e+03, 1.47910e+03, 2.29305e+03, 8.38835e-01, 0.00000e+00],\n",
       "        [1.33620e+03, 1.30665e+03, 1.38581e+03, 1.38278e+03, 8.38572e-01, 0.00000e+00],\n",
       "        [7.46007e+02, 1.63791e+02, 7.79548e+02, 2.19061e+02, 8.37310e-01, 0.00000e+00],\n",
       "        [1.14234e+03, 7.25872e+02, 1.18160e+03, 7.85555e+02, 8.33452e-01, 0.00000e+00],\n",
       "        [1.28350e+03, 1.69889e+03, 1.34053e+03, 1.78646e+03, 8.31392e-01, 0.00000e+00],\n",
       "        [1.20805e+03, 1.23356e+03, 1.26233e+03, 1.31258e+03, 8.31268e-01, 0.00000e+00],\n",
       "        [6.81445e+02, 5.70501e+02, 7.24268e+02, 6.32826e+02, 8.30987e-01, 0.00000e+00],\n",
       "        [1.21193e+03, 1.25789e+02, 1.25339e+03, 1.90748e+02, 8.28833e-01, 0.00000e+00],\n",
       "        [1.33997e+03, 1.54883e+03, 1.39766e+03, 1.63437e+03, 8.28785e-01, 0.00000e+00],\n",
       "        [1.54749e+03, 2.23149e+03, 1.60175e+03, 2.30905e+03, 8.25733e-01, 0.00000e+00],\n",
       "        [8.52919e+02, 2.11543e+03, 9.05219e+02, 2.19237e+03, 8.25538e-01, 0.00000e+00],\n",
       "        [1.12275e+03, 8.10812e+02, 1.15636e+03, 8.57932e+02, 7.92192e-01, 0.00000e+00],\n",
       "        [7.37790e+02, 2.20819e+03, 7.74526e+02, 2.26228e+03, 7.89918e-01, 0.00000e+00]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "thousand-cisco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.45282e+03, 1.45009e+03, 1.50923e+03, 1.53559e+03, 8.58536e-01, 0.00000e+00],\n",
       "         [8.11149e+02, 2.51468e+02, 8.65843e+02, 3.34357e+02, 8.56946e-01, 0.00000e+00],\n",
       "         [1.32890e+03, 6.14181e+02, 1.38540e+03, 6.95824e+02, 8.49743e-01, 0.00000e+00],\n",
       "         [1.32848e+03, 1.39324e+03, 1.36927e+03, 1.45567e+03, 8.44309e-01, 0.00000e+00],\n",
       "         [8.95427e+02, 1.55035e+02, 9.40290e+02, 2.22714e+02, 8.43017e-01, 0.00000e+00],\n",
       "         [8.04034e+02, 4.29946e+02, 8.54911e+02, 5.09081e+02, 8.40511e-01, 0.00000e+00],\n",
       "         [1.42461e+03, 2.20768e+03, 1.47910e+03, 2.29305e+03, 8.38835e-01, 0.00000e+00],\n",
       "         [1.33620e+03, 1.30665e+03, 1.38581e+03, 1.38278e+03, 8.38572e-01, 0.00000e+00],\n",
       "         [7.46007e+02, 1.63791e+02, 7.79548e+02, 2.19061e+02, 8.37310e-01, 0.00000e+00],\n",
       "         [1.14234e+03, 7.25872e+02, 1.18160e+03, 7.85555e+02, 8.33452e-01, 0.00000e+00],\n",
       "         [1.28350e+03, 1.69889e+03, 1.34053e+03, 1.78646e+03, 8.31392e-01, 0.00000e+00],\n",
       "         [1.20805e+03, 1.23356e+03, 1.26233e+03, 1.31258e+03, 8.31268e-01, 0.00000e+00],\n",
       "         [6.81445e+02, 5.70501e+02, 7.24268e+02, 6.32826e+02, 8.30987e-01, 0.00000e+00],\n",
       "         [1.21193e+03, 1.25789e+02, 1.25339e+03, 1.90748e+02, 8.28833e-01, 0.00000e+00],\n",
       "         [1.33997e+03, 1.54883e+03, 1.39766e+03, 1.63437e+03, 8.28785e-01, 0.00000e+00],\n",
       "         [1.54749e+03, 2.23149e+03, 1.60175e+03, 2.30905e+03, 8.25733e-01, 0.00000e+00],\n",
       "         [8.52919e+02, 2.11543e+03, 9.05219e+02, 2.19237e+03, 8.25538e-01, 0.00000e+00],\n",
       "         [1.12275e+03, 8.10812e+02, 1.15636e+03, 8.57932e+02, 7.92192e-01, 0.00000e+00],\n",
       "         [7.37790e+02, 2.20819e+03, 7.74526e+02, 2.26228e+03, 7.89918e-01, 0.00000e+00],\n",
       "         [1.23392e+03, 1.27033e+03, 1.23707e+03, 1.27623e+03, 6.64147e-01, 0.00000e+00],\n",
       "         [8.27529e+02, 4.66882e+02, 8.30594e+02, 4.72392e+02, 6.63699e-01, 0.00000e+00],\n",
       "         [1.35932e+03, 1.34197e+03, 1.36236e+03, 1.34754e+03, 6.58507e-01, 0.00000e+00],\n",
       "         [8.77805e+02, 2.15140e+03, 8.80863e+02, 2.15687e+03, 6.54635e-01, 0.00000e+00],\n",
       "         [1.57291e+03, 2.26772e+03, 1.57595e+03, 2.27312e+03, 6.52650e-01, 0.00000e+00],\n",
       "         [1.35525e+03, 6.52543e+02, 1.35833e+03, 6.58060e+02, 6.41166e-01, 0.00000e+00],\n",
       "         [1.45012e+03, 2.24749e+03, 1.45353e+03, 2.25408e+03, 6.34566e-01, 0.00000e+00],\n",
       "         [8.37390e+02, 2.89459e+02, 8.40605e+02, 2.95434e+02, 6.31863e-01, 0.00000e+00],\n",
       "         [1.47993e+03, 1.48860e+03, 1.48322e+03, 1.49462e+03, 6.27043e-01, 0.00000e+00],\n",
       "         [1.31047e+03, 1.74061e+03, 1.31396e+03, 1.74714e+03, 6.23834e-01, 0.00000e+00],\n",
       "         [1.36719e+03, 1.58846e+03, 1.37050e+03, 1.59472e+03, 6.18528e-01, 0.00000e+00],\n",
       "         [1.23135e+03, 1.56254e+02, 1.23416e+03, 1.61483e+02, 6.11631e-01, 0.00000e+00],\n",
       "         [1.34732e+03, 1.42185e+03, 1.35014e+03, 1.42707e+03, 6.06137e-01, 0.00000e+00],\n",
       "         [7.00657e+02, 6.00105e+02, 7.03530e+02, 6.05261e+02, 6.00729e-01, 0.00000e+00],\n",
       "         [9.16384e+02, 1.85675e+02, 9.19341e+02, 1.91016e+02, 5.99634e-01, 0.00000e+00],\n",
       "         [1.16060e+03, 7.53294e+02, 1.16341e+03, 7.58378e+02, 5.64768e-01, 0.00000e+00]])]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-relay",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
